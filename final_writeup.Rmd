---
output:
 pdf_document:
  md_extension: +raw_tex
  latex_engine: xelatex
bibliography: citations.bib
header-includes:
 
# Set 12-point font.

- \usepackage[fontsize = 12pt]{scrextend}

# Set double spacing.
# - \usepackage{setspace}

# Display Chinese characters.

- \usepackage{xeCJK}

# Set the paragraph indent and spacing.
- \usepackage{sectsty}
- \setlength{\parindent}{2em}
- \setlength{\parskip}{1em}

# For displaying the table.
- \usepackage{booktabs}
- \usepackage{graphicx}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(extrafont)
library(lubridate)
library(xtable)
```

\begin{centering}
{\Large 
Unsupervised Text Classification With Chinese Social Media

An Extension of King, Pan, and Roberts (2017)

}
\end{centering}

\begin{flushright}
Gabe Walker

\vspace{-.5em}

May 12, 2019

\vspace{-.5em}

Gov 1006: Models
\end{flushright}


## Abstract

\indent

This paper replicates and extends “How the Chinese Government Fabricates Social Media Posts for Strategic Distraction, Not Engaged Argument” by @kpr. It uses an updated version of the readme software (“readme2”) by @jerzak in conjunction with latent Dirichlet allocation (LDA), a method for unsupervised text classification, to test readme2’s performance on an automatically classified set of Chinese social media posts. Using the replication data provided on the Harvard Dataverse[^dataverse] and the original five-topic classification scheme, it finds strong support for the authors’ original conclusions. It also proposes a new approach for automatically estimating classifications in large corpora when total classification is infeasible with certain machine learning methods.

[^dataverse]: @dataverse

## I. Introduction

\indent

The “50 Cent Party” (五毛党) is the commonly used collective term for “internet commentators” (网络评论员), individuals who post pro-government content across the Chinese web.[^note1] Their activities form an important part of the Chinese government’s “public opinion guidance” system, a holistic approach to information control that spans all forms of print, television, and digital media. In the era of WeChat and Weibo, with around 1.5 billion members between the two, public opinion guidance is more important than ever. Over the past decade, journalists, scholars, and the public have closely followed the activities of the 50 Cent Party as a way to understand the Chinese government’s approach to managing the country’s domestic internet. But because of the opaque and diffuse nature of the Party’s operations, it has been a perennial challenge to describe the specific methods that these commentators use.

[^note1]: The name “50 Cent” comes from the widely held belief that commentators get paid 0.50 RMB per post. 

In "How the Chinese Government Fabricates Social Media Posts for Strategic Distraction, Not Engaged Argument," @kpr conducted the first large-scale empirical analysis of the 50 Cent Party’s operations. Using nearly 44,000 leaked web posts from the Zhanggong Internet Propaganda Office in Jiangxi Province, they characterize the timing and content of 50 Cent Party activity on the site. They then extrapolate their findings to China as a whole. Overall, they conclude that “prevailing views” about the 50 Cent Party are “largely incorrect.” Previously, the majority of journalists and academics believed that Party members posted nationalistic and pro-government content and argumentatively tackled controversial debates head-on. King, Pan, and Roberts find, however, that these commentators often post positive, “cheerleading” content that avoids touchy subjects and outright criticism. They also find that 50 Cent commentators coordinate the timing and content of their activities and probably publish around 450 million posts a year across the Chinese web.

This paper aims to replicate King, Pan, and Roberts’s results and to employ their methods in a new way. Specifically, I use readme2, an improved software for proportion estimation by @jerzak to reproduce their results, and a fully automated clustering approach in concert with readme2 to “automate” proportion estimation. I find that my results align closely with those reported in the original paper and that there may be interesting potential to use readme2 to estimate proportions in large document corpora without human assistance.


## II. Replication

\indent

In order to replicate some of the results in @kpr, I obtained the paper’s original datasets of collected posts from the Harvard Dataverse. The first set of interest is the 43,757 known 50 Cent Party posts mentioned in the leaked files from the Zhanggong Internet Propaganda Office. These posts date from 2013 and 2014 and appear on a wide variety of different Chinese websites, including social media, discussion forums, and government-run sites. More than half of those on commercial sites appeared on Weibo. The second set of interest is scraped Weibo posts from “exclusive” sources named in the Zhanggong leak: that is, a collection of 5,584 posts from accounts that almost never post anything *besides* 50 Cent Party content. I chose these two accounts to attempt to replicate a portion of the original findings. The one difference in my approach is that I use readme2, and updated version of the readme software the authors used in their 2017 publication.

\pagebreak

First, I show that the timing of the known posts matches up exactly with what the authors reported. See the appendix for the reproduced original figure (Figure 2) from the paper.

```{r timeline, echo=FALSE, message=FALSE, error=FALSE, fig.align='center'}

# Read in the known 50c data.

df <- read_rds("results/df.rds")

# Make a tibble of dates.

labels <- tibble(
  event = c(
    "Qingming Festival \n (April)",
    "China \n Dream \n (May)",
    "Shanshan \n Riots \n (July)",
    "Third \n Plenum \n (November)",
    "Two \n meetings \n (February)",
    "Urumqi rail explosion \n (May)",
    "Martyr's Day \n (October)"
  ),
  date = c(
    date("2013-07-01"),
    date("2013-05-20"),
    date("2013-08-10"),
    date("2013-11-01"),
    date("2014-02-05"),
    date("2014-05-01"),
    date("2014-08-05")
  ),
  y = c(
    3000,
    1600,
    1000,
    600,
    1000,
    2200,
    3000
  )
)

ggplot(df, aes(x = date)) +
  geom_density(stat = "count") +
  theme(
    text = element_text(size = 14, family = "LM Roman 10"),
    panel.background = element_blank(),
    legend.key = element_blank()
  ) +
  labs(
    y = "Count of posts",
    x = "Date (January 2013–December 2014)",
    title = "Fig. 1: Time series of 43,757 known \n 50 Cent Party posts."
  ) +
  geom_text(
    data = labels, aes(label = event, color = "red4", fontface = 2),
    y = labels$y,
    x = labels$date,
    family = "LM Roman 10",
    show.legend = FALSE
  )
```

\vspace{-3em}

\begin{center}
\footnotesize{Figure 1: The peaks show counts of identified 50 Cent Party posts during significant events (noted in red) during 2013–2014.}
\end{center}

Next, I use readme2 to estimate the proportions of the different categories in the first set of 43,757 posts. Both readme and readme2 work as described in @hopkins by using human-determined categories to estimate the proportions of those categories in the entire corpus. For example, in this case the authors initially chose five possible categories of posts.[^note2] Then two Chinese speakers were provided with 200 randomly selected posts (of the 43,757) to sort. The 188 that they agreed upon are then fed into readme, which provides an unbiased estimate of the proportions of each category in the *entire* collection of posts. The key distinction of this approach compared to many other methods of text classification is that readme does *not* actually classify anything. readme2 operates in the same way with some new improvements as described in @jerzak. The process for estimating proportions for the exclusive posts is exactly the same.

\pagebreak

The mean results of three runs of readme2 on the original datasets (leaked and exclusive posts), juxtaposed with results from the original paper, show close alignment between the two.[^note3]

[^note2]: Taunting of foreign countries; argumentative praise or criticism; nonargumentative praise or criticism; factual reporting; cheerleading for China.
[^note3]: These are results I estimated from Figure 3 of the paper, reproduced in the appendix.

\vspace{-2em}

```{r proportions, echo=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.dim=c(9,7)}

# The replicated main results using readme2.

# First read in the initial data.

results_original_table <- read_rds("results/results_original_table.rds")
results_exclusive_table <- read_rds("results/results_exclusive_table.rds")

# The content proportion graph.

means <- c(0,
           0,
           mean(results_original_table$`3`),
           mean(results_original_table$`5`),
           mean(results_original_table$`4`),
           0,
           mean(results_exclusive_table$`Argumentative praise or criticism`),
           mean(results_exclusive_table$`Non-argumentative Praise or Suggestions`),
           mean(results_exclusive_table$`Factual Reporting`),
           mean(results_exclusive_table$`Cheerleading for China`),
           0,
           0,
           .16,
           .08,
           .8,
           0,
           .06,
           .31,
           .19,
           .45)

# Upper and lower confidence intervals.
# Original ones from the paper are eyeballed estimates.

lower <- c(0,
           0,
           mean(results_original_table$`3`) - 1.96/sqrt(3)*sd(results_original_table$`3`),
           mean(results_original_table$`5`) - 1.96/sqrt(3)*sd(results_original_table$`5`),
           mean(results_original_table$`4`) - 1.96/sqrt(3)*sd(results_original_table$`4`),
           0,
           mean(results_exclusive_table$`Argumentative praise or criticism`) - 1.96/sqrt(3)*sd(results_exclusive_table$`Argumentative praise or criticism`),
           mean(results_exclusive_table$`Non-argumentative Praise or Suggestions`) - 1.96/sqrt(3)*sd(results_exclusive_table$`Non-argumentative Praise or Suggestions`),
           mean(results_exclusive_table$`Factual Reporting`) - 1.96/sqrt(3)*sd(results_exclusive_table$`Factual Reporting`),
           mean(results_exclusive_table$`Cheerleading for China`) - 1.96/sqrt(3)*sd(results_exclusive_table$`Cheerleading for China`),
           0,
           0,
           .08,
           .02,
           .7,
           0,
           0,
           .22,
           .13,
           .4)

upper <- c(0.0025,
           0.0025,
           mean(results_original_table$`3`) + 1.96/sqrt(3)*sd(results_original_table$`3`),
           mean(results_original_table$`5`) + 1.96/sqrt(3)*sd(results_original_table$`5`),
           mean(results_original_table$`4`) + 1.96/sqrt(3)*sd(results_original_table$`4`),
           0.0025,
           mean(results_exclusive_table$`Argumentative praise or criticism`) + 1.96/sqrt(3)*sd(results_exclusive_table$`Argumentative praise or criticism`),
           mean(results_exclusive_table$`Non-argumentative Praise or Suggestions`) + 1.96/sqrt(3)*sd(results_exclusive_table$`Non-argumentative Praise or Suggestions`),
           mean(results_exclusive_table$`Factual Reporting`) + 1.96/sqrt(3)*sd(results_exclusive_table$`Factual Reporting`),
           mean(results_exclusive_table$`Cheerleading for China`) + 1.96/sqrt(3)*sd(results_exclusive_table$`Cheerleading for China`),
           .0025,
           .0025,
           .21,
           .15,
           .88,
           .0025,
           .1,
           .4,
           .25,
           .5)

groups <- c(rep("Leaked", 5), rep("Exclusive", 5), rep("Leaked (original)", 5), rep("Exclusive (original)", 5))

tibble(
  category = rep(c("Taunting of foreign countries", "Argumentative praise or criticism", "Nonargumentative praise or suggestions", "Factual reporting", "Cheerleading"), 4),
  mean = means,
  lower = lower,
  upper = upper,
  group = groups,
  x = c(.9, 1.9, 2.9, 3.9, 4.9, 1, 2, 3, 4, 5, 1.1, 2.1, 3.1, 4.1, 5.1, 1.2, 2.2, 3.2, 4.2, 5.2)
) %>% 

# Create the graph.

  ggplot(aes(x, mean, col = group)) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = .3) +
  geom_point(aes(col = group), size = 4) +
  geom_segment(aes(x = x, y = lower, xend = x, yend = upper), size = 1, alpha = .5) +
  scale_color_discrete(name = "Post source") +
  scale_x_discrete(limits = c(1, 2, 3, 4, 5),
                   labels = c("1" = "Taunting of \n foreign countries",
                              "2" = "Argumentative praise \n or criticism",
                              "3" = "Nonargumentative praise \n or suggestions",
                              "4" = "Factual reporting",
                              "5" = "Cheerleading")) +
  theme(
    text = element_text(size = 16, family = "LM Roman 10"),
    panel.background = element_blank(),
    axis.title.x = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.ticks.x = element_blank(),
    legend.key = element_blank(),
    legend.position = c(.25, .65)) +
  labs(y = "Estimated proportion of total posts",
    title = "Fig. 2: The readme2 results match well with those \n from the original paper.")
```

\vspace{-3em}

\begin{center}
\footnotesize{Figure 2: Each color corresponds to a run of readme2. Estimates of the proportions of each of the five categories are given on the vertical axis, with error bars around estimates from the original paper. Leaked posts number 43,757 and posts from "exclusive" 50 Cent Party accounts number 5,584.}
\end{center}

As evident in Fig. 2, nearly all of my results match well with those from the original paper. None of the human-coded posts included examples of “taunting of foreign countries,” so readme ignores that category completely. The leaked posts had no “argumentative praise or criticism” either—so in this case readme essentially provided estimates of only *three* categories and assumed the others were zero. For the other three categories, my readme2 results and the original results align within the margin of error given in Figure 3 of the paper. A greater number of repeated trials of readme2 would have provided a confidence interval around the mean estimates (i.e., the points on the graph), but the obvious similarity would have remained.

Overall, both results suggest that 50 Cent Party posts in these samples are primarily “cheerleading for China” (between 50 and 75 percent) and “nonargumentative praise or suggestions” and “factual reporting” (around 30–50 percent together). Very few, if any, are “argumentative” or “taunting of foreign countries.”

## III. Extension

\indent

A few aspects of @kpr prompted me to apply their methods in a new way. First, the paper’s choice of five categories seemed somewhat arbitrary. For example, this post is an example of one classified as “argumentative praise or criticism”:

> Lee Kai-Fu says that you can buy a villa for $600,000 USD in New York, much cheaper than in Beijing. But what he doesn’t tell you is that this so-called villa is actually a warehouse, which is more than a four-hour drive from New York City.

> 李开复说纽约60万美金一套别野，比北京便宜多了，但他不胡告诉你那套所谓的别野其实是个仓库，而且离纽约市区车需要四个多小时。

This post clearly aligns with the authors’ definition because it includes criticism of an individual. But it also seems to taunt the United States and may even have elements of factual reporting. What would happen if we defined starting categories that were completely different than these five? And would readme faithfully return unbiased estimates of these new categories?

In the first step of my extension I implement a different approach to classifying the known 50 Cent Party posts. Rather than initially hand-coding a subset of posts I use latent Dirichlet allocation (LDA), just one of more than 150 fully automated classification techniques reported in @grimmer, to cluster posts. To do this, I selected a subset of 5,000 of the 43,757 (to save computation time) to transform into a “document term matrix,” a numeric array with the frequencies of every word contained in the corpus. After eliminating sparse terms (that appeared in fewer than 1 in every 1,000 posts) and sparse posts (that did not contain any common terms), I applied a basic LDA model without any specified hyperparameters to identify five distinct topics.[^note4]

[^note4]: Only to keep some general equivalence to the original paper. Note that because the objective functions of different automated clustering approaches are totally different, this LDA outcome is just one out of many possible classification schemes that we might expect to see. And five categories is in this case an arbitrary choice without any measures of purity within each group.

\pagebreak

The table below contains the top eight terms of each of five categories, my names for the categories, and the proportion of each in the 5,000 posts.

\begin{centering}
\begin{tabular}{@{}p{1cm}p{2cm}p{1.75cm}p{6.5cm}p{3.5cm}@{}}
\midrule
Topic & Name                                            & Proportion & Translation    & Terms \\ \midrule
1     & Local Progress   & 0.20       & Ganzhou, the masses, hope, Southern Ganzhou, government, development, two, establish           & 赣州, 群众, 希望, 赣南, 政府, 发展, 两, 建设  \\
2     & National Development    & 0.15       & China, reform, dream, development, society, realize, economy, Party       & 中国, 改革, 梦, 发展, 社会, 实现, 经济, 党                       \\
3     & Ancestral Memory   & 0.14       & Qingming, Qingming Festival, martyr, heroic martyr, cherish, give thanks, memorialize, civilized & 清明, 清明节, 先烈, 英烈, 缅怀, 感恩, 祭奠, 文明 \\
4     & Red Spirit (present)  & 0.26       & revolution, the people, martyr, lucky, the homeland, life, hero, martyr  & 革命, 人民, 烈士, 幸福, 祖国, 生活, 英雄, 先烈                        \\
5     & Red Spirit (past)   & 0.25       & martyr, revolution, spirit, soviet, cherish, revitalize, development, Southern Ganzhou         & 先烈, 革命, 精神, 苏区, 缅怀, 振兴, 发展, 赣南  \\ \midrule
\end{tabular}
\end{centering}

```{r table, echo=FALSE, message=FALSE, error=FALSE, fig.align='center', eval=FALSE}

# Make a table of the top eight words.

top_terms <- read_rds("results/top_terms.rds")
lda_percents <- read_rds("results/lda_percents.rds")

terms <- tibble(topic = seq(1:5), terms = NA, translation = NA)

for (n in seq(1:5)) {
  subset <- top_terms %>% 
    filter(topic == n)
  
  terms$terms[n] <- paste0(subset$term, collapse = ", ")
  terms$translation[n] <- paste0(subset$translation, collapse = ", ")
}

# Make a printable dataframe.

tibble(Topic = c(seq(1:5)),
       Name = c("Local Progress", "National Development", "Ancestral Memory", "Red Spirit (present)", "Red Spirit (past)"),
       Terms = terms$terms,
       Proportion = round(lda_percents$percent, 2),
       Translation = terms$translation) %>% 

# Print the table; the Chinese characters don't work with Kable.
  
  # kable(format = "latex", booktabs = TRUE) %>% 
  # kable_styling(position = "center", full_width = FALSE, font_size = 10)
```

As is clear from the top terms, distinguishing between each category is not easy. This could be in part due to the nature of LDA, which assigns posts to multiple topics and words to multiple topics; that is, there is natural cross-listing between topics and key words. In this case, a few categories share “martyr,” “revolution,” and “development.” It is also possible that five categories is a poor number for this subset of the known 50 Cent Party posts; a greater number might differentiate between topics more clearly. I also attempted the same analysis with a biterm LDA—using pairs of adjacent words—but the result was no more distinct. Regardless of the final real-world significance of these categories, the algorithm did successfully sort the posts, which lays the groundwork for the next step of my extension.

\pagebreak

To test readme2’s performance I ran 45 trials of readme2 “seeded” with 188 randomly selected categorized posts from the 5,000 subset. Again, this number is somewhat arbitrary, and could be increased for greater accuracy. The results show that readme2 estimates accurate proportions of the LDA classifications in the overall document.


```{r extension, echo=FALSE, message=FALSE, error=FALSE, fig.align='center'}

# Plot the extension chart.

# First load the necessary data.

lda_percents <- read_rds("results/lda_percents.rds")

results_seeded_table <- read_rds("results/results_seeded_table.rds")

# Creat the necessary dataframes.

lda_results <- tibble(proportion = lda_percents$percent, group = lda_percents$value)

seed_plot <- results_seeded_table %>% 
  gather(key = "group", value = "proportion", `1`:`5`) %>% 
  rename(iteration = n) %>% 
  mutate(group = factor(case_when(
    group == 1 ~ "Local \n Progress",
    group == 2 ~ "National \n Development",
    group == 3 ~ "Ancestral \n Memory",
    group == 4 ~ "Red Spirit \n (present)",
    group == 5 ~ "Red Spirit \n (past)",
  )))

# Density plot.

ggplot(seed_plot, aes(proportion, group = group, fill = group, col = group)) +
  geom_density(alpha = .3) +
  geom_vline(xintercept = lda_results$proportion,
             col = c("#A3A500", "#00BF7D", "#F8766D", "#E76BF3", "#00B0F6"),
             linetype = "dashed",
             alpha = .75,
             size = 1,
             show.legend = FALSE) +
  theme(
    text = element_text(size = 16, family = "LM Roman 10"),
    panel.background = element_blank(),
    legend.key = element_blank(),
    legend.title = element_blank(),
    legend.position = c(.90, .8)) +
  labs(y = "Density",
       x = "Proportion",
       title = "Fig. 3: readme2 provides accurate estimates \n of LDA classification proportions.") +
  guides(fill = guide_legend(title = "Class"),
         col = "none")
```

\vspace{-3em}

\begin{center}
\footnotesize{Figure 3: The colored distributions are estimates of the proportions of each group in the 5,000-post subset from 45 readme2 runs. The dotted lines are the actual proportions as determined by LDA.}
\end{center}

Fig. 3 shows that readme2 provides excellent estimates of the actual proportions that LDA classified (shown with the dotted lines). It even captures the difference between “national development” and “ancestral memory,” which differ by only around 1 percent. However, though the means are not statistically different (with a one-sample t-test), there is some variance to the individual readme outputs, likely based on the initial “seed” proportions. For example, readme2’s maximum and minimum estimates for “Red Spirit (present)” are 31 and 17 percent, respectively, with a mean of 24.7 percent (actual is 24.9).

In one respect this test of readme2 is a meaningless exercise: LDA already classified all of the subsample posts so we do not need to know the “actual” proportions. In another respect, however, it may have future usefulness. Some unsupervised classification methods, such as hierarchical clustering on a high-dimensional document feature matrix, are challenging to compute on some machines. The above results suggest that readme2 may provide a way to “extrapolate” proportions from a small set to a large set. For example, the unsupervised approach could label a small sample set, “seed” readme2 with those, and then have readme2 repeatedly estimate the proportions in the much larger dataset. This could provide a useful workaround for directly classifying each and every document in a large corpus and an interesting exploratory step for automatically surveying the landscape of an expansive collection of documents.

## Acknowledgements

Many thanks to David Kane and Mark Hill for their enthusiasm and support, and to Gary King for his helpful suggestions.

\pagebreak

## Appendix: Replicated Figures

### Figure 2: Time Series of 43,757 Known 50c Social Media Posts with Qualitative Summaries of the Content of Volume Bursts

```{r timeline_original, echo=FALSE, message=FALSE, error=FALSE, fig.align='center', fig.dim=c(10,7)}

# Reproduce the original timeline from the paper, figure 2.

posts <- read_csv("data/posts_all.csv")

# Make a table of all the post dates.

postcount <- table(posts$PostDate)

# Make the sequence of dates from the posts.

dateseq <- seq(min(posts$PostDate), max(posts$PostDate), by="day")
dateseq <- dateseq[!dateseq%in%as.Date(names(postcount))]
zeros <- rep(0, length(dateseq))
names(zeros) <- dateseq
postcount <- c(postcount, zeros)
postcount <- postcount[order(as.Date(names(postcount)))]

# Make a list of months for the x-axis labels.

months <- seq(as.Date("2013-01-01"), as.Date("2014-12-01"), by="month")
monthslab <- months(months, abbreviate=T)

# Plot the time plot.

plot(as.Date(names(postcount)),as.vector(postcount), type="l", ylab="Count of Posts",xlab="Date (Jan 2013 - Dec 2014)",xaxt="n",ylim=c(0,4000))

axis(1,at=months,labels=monthslab)

# Add labels for suggested post triggers.

text(as.Date("2013-04-07"),3600,"1. Qingming\nfestival\n(April)", col="red")
text(as.Date("2013-05-23"),1400,"2. China\nDream\n(May)", col="red")
text(as.Date("2013-07-01"),2000,"3. Shanshan\nriots (July)", col="red")
arrows(as.Date("2013-07-01"), 1800, as.Date("2013-07-01"), 1050, col="red", length=0.1)
text(as.Date("2013-11-09"), 600, "4. 3rd plenum\nCCP 18th\nCongress (Nov)", col="red")
text(as.Date("2014-02-12"), 1150, "5. Two meetings\n(Feb)", col="red")
arrows(as.Date("2014-02-12"), 950, as.Date("2014-02-12"), 600, col="red", length=0.1)
text(as.Date("2014-05-09"),2200,"6. Urumqi rail\nexplosion (May)", col="red")
text(as.Date("2014-07-15"),1300,"7. Gov't\nforum,\npraise\ncentral\nsubsidy\n(Jul-Aug)", col="red")

segments(as.Date("2014-07-15"),750,as.Date("2014-07-15"),650, col="red")
segments(as.Date("2014-06-15"),650,as.Date("2014-08-30"), col="red")
segments(as.Date("2014-06-15"),650,as.Date("2014-06-15"),600, col="red")
segments(as.Date("2014-08-30"),650,as.Date("2014-08-30"),600, col="red")
text(as.Date("2014-10-08"),3000,"8. Martyr's\nDay\n(Oct)", col="red", pos=2)
```

### Figure 3. Content of Leaked and Inferred 50c Posts, by Substantive Category

```{r fig3data, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, cache=TRUE, results=FALSE}

# Figure 3.
# Results of the ReadMe analysis.

# Read in all the CSVs for the figure.
# Note that the replication code changes the working directory and this does not.

results1 <- read.csv("data/figure_3/Analysis1/ReadMeBootResults.csv")
results2 <- read.csv("data/figure_3/Analysis2/ReadMeBootResults.csv")
selectresults <- read.csv("data/figure_3/Analysis2/ReadMeBootResults_Exclusive.csv")
notselectresults <- read.csv("data/figure_3/Analysis2/ReadMeBootResults_Ordinary.csv")
results3 <- read.csv("data/figure_3/Analysis3/ReadMeBootResults.csv")
results4 <- read.csv("data/figure_3/Analysis4/ReadMeBootResults.csv")

# Normalize to remove Other category (Category 2).
# Note that none of the variables have names (!).

results1 <- results1/(1-results1[,2])
results2 <- results2/(1-results2[,2])
results3 <- results3/(1-results3[,2])
results4 <- results4/(1-results4[,2])
selectresults <- selectresults/(1-selectresults[,2])
notselectresults <- notselectresults/(1-notselectresults[,2])

# Make point estimates for each category.

# Analysis 1
apply(results1[,c(1,3:6)],2,mean)
# Analysis 2
apply(results2[,c(1,3:6)],2,mean)
# Ordinary
apply(notselectresults[,c(1,3:6)],2,mean)
# Exclusive
apply(selectresults[,c(1,3:6)],2,mean)
# Analysis 3
apply(results3[,c(1,3:6)],2,mean)
# Analysis 4
apply(results4[,c(1,3:6)],2,mean)

```

```{r fig3plot, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE, fig.dim=c(10,7)}

# Plot the results.

plot(c(1,3,5,4,2), apply(results1[,c(1,3:6)], 2, mean), xaxt = "n", ylim = c(0,1), xlab="",
     ylab="Proportion", pch=16, xlim=c(0.5,6.1))

segments(c(1,3,5,4,2), apply(results1[,c(1,3:6)],2, function (x)
    quantile(x,.975)), c(1,3,5,4,2),apply(results1[,c(1,3:6)],2, function (x)
      quantile(x,.025)))

points(c(1,3,5,4,2)+.1, apply(results2[,c(1,3:6)],2,mean), pch=17)
segments(c(1,3,5,4,2) +.1, apply(results2[,c(1,3:6)],2, function (x)
  quantile(x,.975)), c(1,3,5,4,2)+.1,apply(results2[,c(1,3:6)],2, function (x)
    quantile(x,.025)), lty=2)

points(c(1,3,5,4,2)+.2, apply(selectresults[,c(1,3:6)],2,mean), pch=7, col="red")
segments(c(1,3,5,4,2) +.2, apply(selectresults[,c(1,3:6)],2, function (x)
  quantile(x,.975)), c(1,3,5,4,2)+.2,apply(selectresults[,c(1,3:6)],2, function (x)
    quantile(x,.025)), lty=2, col="red")

points(c(1,3,5,4,2)+.3, apply(notselectresults[,c(1,3:6)],2,mean), pch=8, col="red")
segments(c(1,3,5,4,2) +.3, apply(notselectresults[,c(1,3:6)],2, function (x)
  quantile(x,.975)), c(1,3,5,4,2)+.3,apply(notselectresults[,c(1,3:6)],2, function (x)
    quantile(x,.025)), lty=2, col="red")

points(c(1,3,5,4,2)+.4, apply(results3[,c(1,3:6)],2,mean), pch=4)
segments(c(1,3,5,4,2) +.4, apply(results3[,c(1,3:6)],2, function (x)
  quantile(x,.975)), c(1,3,5,4,2)+.4,apply(results3[,c(1,3:6)],2, function (x)
    quantile(x,.025)), lty=3)

points(c(1,3,5,4,2)+.5, apply(results4[,c(1,3:6)],2,mean), pch=5)
segments(c(1,3,5,4,2) +.5, apply(results4[,c(1,3:6)],2, function (x)
  quantile(x,.975)), c(1,3,5,4,2)+.5,apply(results4[,c(1,3:6)],2, function (x)
    quantile(x,.025)), lty=5)

# Write the legend.

legend(.4,1, c("Leaked e-mails, all sites", "Leaked accounts, Weibo",
                "Leaked accounts, exclusive",
                "Leaked accounts, ordinary", "Within county prediction, all posts",  "Out of county prediction"),
       lty=c(1,2,2,2,3,4), pch=c(16,17,7,8,4,5), col=c(rep("black",2),
                                                       rep("red",2), rep("black",2)))

text(1.8, .17, "Argumentative Praise \n or Criticism")
text(.9, 0.08, "Taunting of Foreign \n Countries")
text(5.5, .8, "Cheerleading")
text(4.8, .17, "Factual \n Reporting")
text(3.2, 0, "Non-argumentative \n Praise or Suggestions")
```

\pagebreak

## References

\indent