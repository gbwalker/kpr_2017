{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf810
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset134 STHeitiSC-Light;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww14960\viewh13540\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 #Notes for the replication file:\
\
#Leaked archive: available upon request, or accessible from Internet Archive here:  {\field{\*\fldinst{HYPERLINK "https://web.archive.org/web/20160331215004/http://xiaolan.me/50-cent-party-jxgzzg.html"}}{\fldrslt https://web.archive.org/web/20160331215004/http://xiaolan.me/50-cent-party-jxgzzg.html}}\
#After the archive was unzipped 
\f1 \'d2\'d1\'b7\'a2\'cb\'cd
\f0  folders were combined into yifasong folder. E-mails in each folder were then numbered so that they were easier to keep track of using the following bash command:\
find . -name '*.eml' | gawk 'BEGIN\{ a=1 \}\{ printf "mv \\"%s\\" %04d.eml\\n", $0, a++\}' | bash\
\
#eml_parser: Contains code to parse .eml files from the leaked archive.\
\
#PostDataSets: posts_all.csv contains all posts that RAs extracted from the leaked e-mail file.\
  # folder: folder from leaked archive of emails where post comes from\
  # What.is.the.name.of.the.organization.making.posts.: name of organization from leaked email\
  # url: specific url of the post from leaked email\
  # content: content of post from leaked email\
  # site: website of post from leaked email\
  # What.is.the.account.name.of.the.person.posting.: account name used to make post\
  # PostDate: estimated date of post, if not available in post data, then based on email date\
  #Category: randomly sampled posts have been assigned a category for the training set.  1 \'97 taunting of foreign countries, 2 - other, 3 - non-argumentative praise and suggestions, 4 - cheerleading for china, 5 - factual reporting, 6 - argumentative praise or criticism\
   #textseg \'97 segmented content, segmented with Stanford Segmenter, version 04-20-2015.\
#postsites.csv contains list of cleaned unique websites where posts were made\
#knownWeibos_zg.csv contains scraped posts of leaked weibo accounts\
    #account_id: weibo account id\
    #status: status comment\
    #date: date of post\
    #attached_url: attached url if relevant\
    #num_likes: number of likes\
    #num_comments: number of comments\
    #num_forwards: number of forwards\
    #forwarded: whether the post was forwarding other text\
    #f_text: if forwarded, the forwarded text\
    #f_date: forwarded text date\
    #f_post_id: forwarded text post ID\
    #f_num_comments: forwarded text num comments\
    #f_num_likes: forwarded text num likes\
    #f_num_forwards: forwarded text num forwards\
    #f_account_id: forwarder account id\
    #Category for classified text\
    #combineposts: used for classification\
    #textseg: combined posts segmented, segmented with Stanford Segmenter, version 04-20-2015.\
    #exclusive: 1 if the account had fewer than 5 followers, 0 if the account had more than 5 followers when checked\
    #datecreated: date that the account was created\
\
#Figure 1: Contains code to replicate the network graph in Figure 1.\
\
#Figure 2: Contains code to replicate the time plot in Figure 2.\
\
#Summary Statistics: Contains code for summary statistics from posts\
\
#ReadMes: Analysis1 contains ReadMe analysis for leaked posts.  \
#Analysis 2  contains ReadMe analysis for leaked accounts posts.\
#Analysis 3 contains ReadMe output for posts from predicted accounts within ZG.\
#Analysis 4 contains ReadMe output for posts from predicted accounts across China.  It also includes the sampling frame of county Weibo\'92s from which 100 were randomly sampled to provide the seed for identifying predicted 50c accounts and posts.\
#ReadMePlot.R contains code for replicating readme plot.}