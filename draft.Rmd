---
# title: "Unsupervised Text Classification With Chinese Social Media: An Extension of King, Pan, and Roberts (2017)"
output:
 pdf_document:
  md_extension: +raw_tex
  latex_engine: xelatex
header-includes:
  
# Set 12-point font.

- \usepackage[fontsize = 12pt]{scrextend}

# Set double spacing.
# - \usepackage{setspace}

# Display Chinese characters.

- \usepackage{xeCJK}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(extrafont)
library(janitor)
library(tm)
library(tmcn)
library(tidytext)
library(stringr)
library(textmineR)
library(quanteda)
library(topicmodels)
```

```{r load_data, message=FALSE, warning=FALSE, eval = FALSE}

# Load the known 50c posts from posts_all, which has 43,757 leaked posts from the Zhanggong Internet Propaganda Department.
# It contains information about the posting organization, some URLs, some account names, post date, and text of the post.

df <- read_csv("data/posts_all.csv") %>% 

# Clean the names of the variables.

  clean_names() %>% 
  mutate(city = what_is_the_city,
         organization = what_is_the_name_of_the_organization_making_posts,
         account = what_is_the_account_name_of_the_person_posting,
         date = post_date,
         text = textseg) %>% 
  select(city, folder, file, organization, url, content, site, account, date, category, text)

# Note that 188 posts are categorized. I presume these are the coded posts that the human coders agreed upon, approximately
# 94 percent of 200 randomly selected posts in total. [N.B., the paper says 93 percent.] It's strange that there are no examples of 1, 2, or 6 (other). How
# does ReadMe know what these look like if there are no examples of them?

# Categories 1 and 2 are "taunting of foreign countries" and "argumentative praise or criticism," respectively.
# Categories 3, 4, and 5 are "non-argumentative praise or suggestions," "factual reporting," and "cheerleading for China," respectively.
# Category 6 is "other." From the paper, "Irrelevant posts that are entirely personal, commercial (such as ads), jokes, or empty posts that forward information not included. This category is removed and conditioned on in all analyses in this article."

#########################
### TEXT MINING
#########################

# Use a comprehensive list of Chinese stopwords 50 percent longer than what's in the tmcn package.
# Downloaded from https://github.com/stopwords-iso/stopwords-zh.

stop_cn <- read_delim("stopwords-zh.txt", delim = "\n", col_names = FALSE) %>% 
  mutate(words = X1) %>% 
  select(words)
  
# Turn the content into a corpus with the tm package.

cp <- Corpus(VectorSource(df$text)) %>% 
  
# Strip out all of the stopwords and punctuation.
  
  tm_map(removePunctuation, ucp = TRUE) %>% 
  tm_map(removeWords, stop_cn[[1]])

# See the cleaned text with this.
# cleaned_text <- tibble(text = get("content", cp))

# Save the corpus into a quanteda corpus object in order to get bigrams.

cp_bi <- corpus(cp)

# Create document term matrices, one with counts, one with tf-idf, and one with bigrams.

dtm <- DocumentTermMatrix(cp)
dtm_tfidf <- DocumentTermMatrix(cp, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
dtm_bi <- dfm(cp_bi, ngrams = 2) %>% 
  convert(to = "tm")

# Remove sparse terms from both, which reduces the words from about 22,000 to 2,000.
# At sparse = .999, the function removes words that appear in less than 1 in every 1,000 documents,
# or in this case 44 documents.
# Worth trying with less granularity at sparse = .99.

dtm_clean <- removeSparseTerms(dtm, .999)
dtm_tfidf_clean <- removeSparseTerms(dtm_tfidf, .999)
dtm_bi_clean <- removeSparseTerms(dtm_bi, .999)

# Eliminate documents without any of the common terms.

rowtotals1 <- apply(dtm_clean, 1, sum)
rowtotals2 <- apply(dtm_tfidf_clean, 1, sum)
rowtotals3 <- apply(dtm_bi_clean, 1, sum)

# The frequency dtm now has 43,572 documents.
# The tfidf has 43,387.
# The bigram has 37,422.

dtm_clean   <- dtm_clean[rowtotals1 > 0, ]
dtm_tfidf_clean   <- dtm_clean[rowtotals2 > 0, ]
dtm_bi_clean   <- dtm_clean[rowtotals3 > 0, ]

# Test LDA.

mod_lda <- LDA(dtm_bi_clean, 5)

# Get information about each post.
# See page 10: https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf

post <- posterior(mod_lda)
terms(mod_lda)
topics <- topics(mod_lda)

# Show the percentage breakdown of the 5 topics.

count(tops, value) %>% mutate(percent = n / nrow(tops))

```

\begin{centering}
{\Large 
DRAFT

Unsupervised Text Classification With Chinese Social Media

An Extension of King, Pan, and Roberts (2017)

}
\end{centering}

## Abstract
<150 words

## I. Introduction
* Background on the 50c party. State of knowledge before KPR (2017) and since.
* Other quantitative approaches to studying the 50c party and Chinese internet “opinion guidance.”
* Summarize King, Pan, and Roberts (2017), specifically data source and results.
* Try to improve figures if I can replicate the original ReadMe results.
* Describe impact/significance of the paper.

## II. Extension

* Motivation/rationale for why the reassessment is warranted: 
  * Categories are informed by existing knowledge but still somewhat arbitrary.
  * Try a few different unsupervised approaches as *inputs* into ReadMe and see if the results differ.
* Define research question: Can ReadMe be fully automated? What categories do different fully unsupervised methods produce, and do they resembled the human-coded ones?
* How unsupervised text classification works (general approach) vis-à-vis ReadMe.
* Why I picked certain methodologies (out of the 150 options presented in King and Grimmer (2011)).

**IDEAS**

* Automate (and possibly bag?) ReadMe results with different unsupervised inputs. See if I can get more precise estimates of the topic proportions than in the paper (or different classifications?).
* Test my results with the out-of-sample batch of 100,000+ posts. (knownWeibos_zg.csv)

## III. Results and Discussion

* What the findings were. [A quick test with LDA alone for 5 categories produced much closer proportions of categories very unlike what's reported in the paper. But these ML models are biased predictors because they aim for classification accuracy, not overall proportion accuracy.]
* Implications of findings.
* Areas for future work.

## Acknowledgements

## References

\pagebreak

## Technical Appendix

* Basically, explaining my code. Why I made certain decisions.

The approach I took is detailed in Stanford (https://web.stanford.edu/~gentzkow/research/text-as-data.pdf, 5). In the document term matrix (DTM), I define $\mathbb{D}$ as the set of individual posts {$\mathbb{D}_i$} with row $c_i$ the numerical vector that represents the presence or weight of a particular language token $j$. I remove stop words (from a list of 750+) and punctuation.

I then reduce the dimensionality of the DTM from around $i \approx 22,000$ to $i \approx 2,000$ with two methods: first by per-document word count, and second by term-frequency-inverse-document-frequency (tf-idf), which excludes both common and rare words.

Term frequency is

$$ tf_{i,j} = \frac{n_{i,j}}{\sum n_{i,j}}$$
where $n_{i,j}$ is the number of occurences of token $j$ in post $i$ and $\sum_k n_{i,j}$ is the total number of occurences of the token through all of the documents.

Inverse data frequency is

$$ idf = \log \bigg(\frac{N}{df_i} \bigg) $$
where $df_i$ is the number of documents containing $i$ out of all of the $N$ documents, so rare words have a high $idf$ score.

Tf-idf is the product of the two terms, $tf \times idf$.

I did not attempt to "stem" the words further because Chinese words do not require stemming (some possible exceptions are nouns and verbs that end in "-们" or "-了," respectively).

I use the "bag-of-words" approach where word order within each post does not matter for the selections reduced with term frequency and tf-idf. I also use a "bigram" (a $n = 2$ $n$-gram) that counts frequencies of adjacent pairs of words.

Next steps: Try out ReadMe (or ReadMe2) and other unsupervised methods from King and Grimmer (2011).